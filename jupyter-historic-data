# DATA MANAGEMENT SYSTEM - JUPYTER NOTEBOOK VERSION
print("DATA MANAGEMENT SYSTEM - JUPYTER NOTEBOOK VERSION")
print("=" * 60)
print("Enterprise data processing system for large datasets")
print("Local Jupyter Notebook environment")
print("Record Types: TYPE_A & TYPE_B")
print("Status Processing: Option1->Status1, Option2->Status2")
print("Processing Status: Value1->Completed, Value2->Rejected, Value3->Not Sent")
print("File format: data_management_system01, 02, 03...")
print("Storage location: Jupyter notebook folder")
print("Excel size: 100K rows per file")
print("Solution: Download issues completely resolved")
print("=" * 60)

import time
import warnings
import json
warnings.filterwarnings('ignore')
GLOBAL_START = time.time()

print("1. Loading libraries...")
try:
    import gspread
    import pandas as pd
    from datetime import datetime
    import concurrent.futures
    import os
    import gc
    from google.oauth2.service_account import Credentials
    print("   Libraries loaded successfully")
except Exception as e:
    print(f"   Library loading error: {e}")
    raise

print("\n2. Google Sheets authentication...")
try:
    # USER CONFIGURATION REQUIRED: Replace with your own Service Account JSON credentials
    # Get your credentials from: https://console.cloud.google.com/apis/credentials
    # Create Service Account -> Download JSON key -> Replace the content below
    service_account_info = {
        "type": "service_account",
        "project_id": "YOUR_PROJECT_ID",           # USER CONFIG: Replace with your Google Cloud Project ID
        "private_key_id": "YOUR_PRIVATE_KEY_ID",   # USER CONFIG: Replace with your Private Key ID
        "private_key": "-----BEGIN PRIVATE KEY-----\nYOUR_PRIVATE_KEY_CONTENT_HERE\n-----END PRIVATE KEY-----\n",  # USER CONFIG: Replace with your Private Key
        "client_email": "YOUR_SERVICE_ACCOUNT@YOUR_PROJECT.iam.gserviceaccount.com",  # USER CONFIG: Replace with your Service Account Email
        "client_id": "YOUR_CLIENT_ID",             # USER CONFIG: Replace with your Client ID
        "auth_uri": "https://accounts.google.com/o/oauth2/auth",
        "token_uri": "https://oauth2.googleapis.com/token",
        "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
        "client_x509_cert_url": "YOUR_CERT_URL",   # USER CONFIG: Replace with your Certificate URL
        "universe_domain": "googleapis.com"
    }
    
    print("   Service Account JSON loaded")
    
    # Create credentials
    SCOPES = ['https://www.googleapis.com/auth/spreadsheets',
              'https://www.googleapis.com/auth/drive']
    creds = Credentials.from_service_account_info(service_account_info, scopes=SCOPES)
    gs_client = gspread.authorize(creds)
    print("   Google Sheets access established successfully")
    
except Exception as e:
    print(f"   Authentication error: {e}")
    print("   SETUP REQUIRED: Please configure your Service Account credentials")
    print("   1. Go to: https://console.cloud.google.com/apis/credentials")
    print("   2. Create Service Account")
    print("   3. Download JSON key")
    print("   4. Replace the service_account_info dictionary above")
    raise

print("\n3. Connecting to source files...")
# USER CONFIGURATION REQUIRED: Replace with your Google Sheets IDs
GOOGLESHEET1_ID = "YOUR_MAIN_SPREADSHEET_ID"      # USER CONFIG: Replace with your main data spreadsheet ID
GOOGLESHEET2_ID = "YOUR_SECONDARY_SPREADSHEET_ID" # USER CONFIG: Replace with your secondary data spreadsheet ID
GOOGLESHEET3_ID = "YOUR_THIRD_SPREADSHEET_ID"     # USER CONFIG: Replace with your third data spreadsheet ID

try:
    main_workbook = gs_client.open_by_key(GOOGLESHEET1_ID)
    secondary_workbook = gs_client.open_by_key(GOOGLESHEET2_ID)
    third_workbook = gs_client.open_by_key(GOOGLESHEET3_ID)
    print("   All Google Sheets files opened successfully")
except Exception as e:
    print(f"   File opening error: {e}")
    print("   PERMISSION REQUIRED: Invite Service Account email to your Sheets:")
    print("   Email: [YOUR_SERVICE_ACCOUNT_EMAIL]")
    print("   Role: Editor")
    raise

print("\n4. System configuration...")
# USER CONFIGURATION: Modify these data source names according to your needs
DATA_SOURCES = ['DATA1', 'DATA2', 'DATA3', 'DATA4', 'DATA5']
SECONDARY_SOURCES = ['SOURCE1', 'SOURCE2', 'SOURCE3', 'SOURCE4', 'SOURCE5', 'SOURCE6', 'SOURCE7']

# USER CONFIGURATION: Modify column mappings according to your spreadsheet structure
DATA_SUBMISSION_SCHEMA = {
    'DATA1': {'key': 0,  'cols': [1, 2, 3, 4]},
    'DATA2': {'key': 6,  'cols': [7, 8, 9, 10]},
    'DATA3': {'key': 12, 'cols': [13, 14, 15, 16]},
    'DATA4': {'key': 18, 'cols': [19, 20, 21, 22]},
    'DATA5': {'key': 24, 'cols': [25, 26, 27, 28]}
}

CANCELLATION_SCHEMA = {
    'DATA1': {'key': 0,  'cols': [1, 2, 3, 4]},
    'DATA2': {'key': 6,  'cols': [7, 8, 9, 10]},
    'DATA4': {'key': 12, 'cols': [13, 14, 15, 16]},
    'DATA3': {'key': 18, 'cols': [19, 20, 21, 22]},
    'DATA5': {'key': 24, 'cols': [25, 26, 27, 28]}
}

SECONDARY_DEFINE_SCHEMA = {
    'DATA1': {'key': 0, 'submission': 1, 'response': 2},
    'SOURCE3': {'key': 4, 'submission': 5, 'response': 6},
    'DATA4': {'key': 8, 'submission': 9, 'response': 10},
    'DATA3': {'key': 12, 'submission': 13, 'response': 14},
    'DATA5': {'key': 16, 'submission': 17, 'response': 18},
    'SOURCE1': {'key': 20, 'submission': 21, 'response': 22},
    'SOURCE2': {'key': 24, 'submission': 25, 'response': 26}
}

def clean_key(value):
    """Clean and normalize key values"""
    return str(value or '').strip().upper()

def clean_text(value):
    """Clean and normalize text values"""
    return str(value or '').strip()

def convert_sales_status(value):
    """Convert sales status values - USER CONFIG: Modify according to your data"""
    value_str = clean_text(value).lower()
    if value_str == 'option_a':  # USER CONFIG: Replace 'option_a' with your actual value
        return 'Status1'
    elif value_str == 'option_b':  # USER CONFIG: Replace 'option_b' with your actual value
        return 'Status2'
    else:
        return clean_text(value) if value else ''

def convert_data_status(value):
    """Convert data status values - USER CONFIG: Modify according to your data"""
    value_str = clean_text(value).lower()
    
    if 'value1' in value_str:  # USER CONFIG: Replace 'value1' with your actual status value
        return 'Completed'
    elif 'value2' in value_str:  # USER CONFIG: Replace 'value2' with your actual status value
        return 'Rejected'
    elif 'value3' in value_str or 'value4' in value_str:  # USER CONFIG: Replace with your waiting status values
        return 'Awaiting Response'
    elif 'value5' in value_str or value_str == '' or not value_str:  # USER CONFIG: Replace with your not sent values
        return 'Not Sent'
    else:
        return clean_text(value) if value else 'Not Sent'

def determine_request_type(cancellation_submission, cancellation_response):
    """Determine request type based on cancellation data"""
    cancellation_submission_str = clean_text(cancellation_submission)
    cancellation_response_str = clean_text(cancellation_response)
    
    if cancellation_submission_str or cancellation_response_str:
        return 'Deletion'
    else:
        return 'Installation'

def combine_date_time(date, time):
    """Combine date and time values into single string"""
    if not date and not time:
        return ''
    
    date_str = clean_text(date)
    time_str = clean_text(time)
    
    if date_str and time_str:
        return f"{date_str} {time_str}"
    elif date_str:
        return f"{date_str} 00:00:00"
    
    return ''

def format_secondary_date(date_str):
    """Format secondary date values"""
    return clean_text(date_str)

def create_excel_file_jupyter(chunk_data, file_name, column_headers):
    """Creates Excel file for Jupyter environment"""
    try:
        print(f"      Creating {file_name}...")
        
        # Create DataFrame
        chunk_df = pd.DataFrame(chunk_data, columns=column_headers)
        print(f"      DataFrame created: {len(chunk_df)} rows")
        
        # Write Excel file
        with pd.ExcelWriter(file_name, engine='xlsxwriter') as writer:
            chunk_df.to_excel(writer, sheet_name='Data Management', index=False)
            
            workbook = writer.book
            worksheet = writer.sheets['Data Management']
            
            # Header format
            header_format = workbook.add_format({
                'bold': True,
                'bg_color': '#2E7D32',
                'font_color': 'white',
                'border': 1,
                'align': 'center',
                'valign': 'vcenter',
                'text_wrap': True
            })
            
            # Write headers
            for col_no, header in enumerate(column_headers):
                worksheet.write(0, col_no, header, header_format)
            
            # USER CONFIG: Modify column widths according to your data
            column_widths = [15, 12, 12, 18, 20, 20, 20, 20, 12, 15, 15, 15, 12, 12, 15, 15, 12, 15, 15, 15]
            for i, width in enumerate(column_widths):
                worksheet.set_column(i, i, width)
            
            # Freeze panes
            worksheet.freeze_panes(1, 0)
            
            # Conditional formatting - USER CONFIG: Modify colors and conditions
            row_count = len(chunk_df)
            
            if row_count > 0:
                # Status coloring
                worksheet.conditional_format(f'D2:D{row_count + 1}', {
                    'type': 'text', 'criteria': 'containing', 'value': 'Completed',
                    'format': workbook.add_format({'bg_color': '#C8E6C9'})
                })
                worksheet.conditional_format(f'D2:D{row_count + 1}', {
                    'type': 'text', 'criteria': 'containing', 'value': 'Rejected',
                    'format': workbook.add_format({'bg_color': '#FFCDD2'})
                })
                worksheet.conditional_format(f'D2:D{row_count + 1}', {
                    'type': 'text', 'criteria': 'containing', 'value': 'Awaiting Response',
                    'format': workbook.add_format({'bg_color': '#FFF9C4'})
                })
                worksheet.conditional_format(f'D2:D{row_count + 1}', {
                    'type': 'text', 'criteria': 'containing', 'value': 'Not Sent',
                    'format': workbook.add_format({'bg_color': '#F0F0F0'})
                })
                
                # Request Type coloring
                worksheet.conditional_format(f'I2:I{row_count + 1}', {
                    'type': 'text', 'criteria': 'containing', 'value': 'Installation',
                    'format': workbook.add_format({'bg_color': '#E3F2FD'})
                })
                worksheet.conditional_format(f'I2:I{row_count + 1}', {
                    'type': 'text', 'criteria': 'containing', 'value': 'Deletion',
                    'format': workbook.add_format({'bg_color': '#FFF3E0'})
                })
                
                # Sales Status coloring
                worksheet.conditional_format(f'J2:J{row_count + 1}', {
                    'type': 'text', 'criteria': 'containing', 'value': 'Status2',
                    'format': workbook.add_format({'bg_color': '#C8E6C9'})
                })
                worksheet.conditional_format(f'J2:J{row_count + 1}', {
                    'type': 'text', 'criteria': 'containing', 'value': 'Status1',
                    'format': workbook.add_format({'bg_color': '#FFCDD2'})
                })
                
                # Type classification coloring
                worksheet.conditional_format(f'K2:K{row_count + 1}', {
                    'type': 'text', 'criteria': 'containing', 'value': 'TYPE_A',
                    'format': workbook.add_format({'bg_color': '#F3E5F5'})
                })
                worksheet.conditional_format(f'K2:K{row_count + 1}', {
                    'type': 'text', 'criteria': 'containing', 'value': 'TYPE_B',
                    'format': workbook.add_format({'bg_color': '#E1F5FE'})
                })
        
        # Calculate file size
        file_size = os.path.getsize(file_name) / 1024 / 1024
        print(f"      {file_name} created successfully ({file_size:.2f} MB)")
        print(f"      File path: {os.path.abspath(file_name)}")
        
        return True, file_size
        
    except Exception as e:
        print(f"      Error creating {file_name}: {e}")
        return False, 0

print("   Configuration completed successfully")

def create_data_management_excel_jupyter():
    """Main function to create comprehensive data management Excel reports"""
    print(f"\nSTARTING JUPYTER DATA MANAGEMENT EXCEL REPORT...")
    print(f"Date: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}")
    print(f"Output: Local Excel files (100K rows per file)")
    print(f"Platform: Jupyter Notebook")
    
    report_start = time.time()
    
    try:
        print("\n5. Parallel data retrieval...")
        data_start = time.time()
        
        def fetch_sheet_data(workbook, sheet_name):
            """Fetch data from Google Sheets worksheet"""
            try:
                worksheet = workbook.worksheet(sheet_name)
                return sheet_name, worksheet.get_all_values()
            except Exception as e:
                print(f"   Warning - {sheet_name} error: {e}")
                return sheet_name, []
        
        # USER CONFIG: Modify sheet names according to your Google Sheets structure
        with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
            tasks = {
                executor.submit(fetch_sheet_data, main_workbook, "MainData"): "DB",                    # USER CONFIG: Replace "MainData" with your sheet name
                executor.submit(fetch_sheet_data, main_workbook, "Submissions"): "BG",                # USER CONFIG: Replace "Submissions" with your sheet name
                executor.submit(fetch_sheet_data, main_workbook, "Cancellations"): "IG",              # USER CONFIG: Replace "Cancellations" with your sheet name
                executor.submit(fetch_sheet_data, secondary_workbook, "DeviceData"): "SECONDARY",     # USER CONFIG: Replace "DeviceData" with your sheet name
                executor.submit(fetch_sheet_data, main_workbook, "MainData"): "TYPE_A_DATA",          # USER CONFIG: Replace with your TYPE_A data sheet name
                executor.submit(fetch_sheet_data, third_workbook, "TypeBData"): "TYPE_B_DB",          # USER CONFIG: Replace "TypeBData" with your sheet name
                executor.submit(fetch_sheet_data, third_workbook, "TypeBSubmissions"): "TYPE_B_DEFINE" # USER CONFIG: Replace "TypeBSubmissions" with your sheet name
            }
            
            raw_data = {}
            for task in concurrent.futures.as_completed(tasks):
                data_type = tasks[task]
                sheet_name, data = task.result()
                raw_data[data_type] = data
                row_count = len(data) - 1 if data else 0
                print(f"   {sheet_name}: {row_count:,} rows")
        
        data_duration = time.time() - data_start
        print(f"   Parallel data retrieval: {data_duration:.1f} seconds")
        
        print("\n6. Fast data processing...")
        processing_start = time.time()
        
        # Process main database data
        print("   Processing main database data...")
        db_mapping = {}
        if len(raw_data['DB']) > 1:
            for row in raw_data['DB'][1:]:
                if len(row) >= 11:
                    # USER CONFIG: Modify column indices according to your data structure
                    record_id = clean_key(row[3])  # Column D (index 3) - modify as needed
                    if record_id:
                        db_mapping[record_id] = {
                            'orgId': clean_text(row[4]),  # Column E (index 4) - modify as needed
                            'DATA4': convert_data_status(row[6]) if len(row) > 6 else 'Not Sent',  # Column G - modify as needed
                            'DATA3': convert_data_status(row[7]) if len(row) > 7 else 'Not Sent',  # Column H - modify as needed
                            'DATA5': convert_data_status(row[8]) if len(row) > 8 else 'Not Sent',  # Column I - modify as needed
                            'DATA1': convert_data_status(row[9]) if len(row) > 9 else 'Not Sent',  # Column J - modify as needed
                            'DATA2': convert_data_status(row[10]) if len(row) > 10 else 'Not Sent' # Column K - modify as needed
                        }
        print(f"      {len(db_mapping):,} main database records processed")
        
        # Process secondary database data
        print("   Processing secondary database data...")
        secondary_db_mapping = {}
        if len(raw_data['TYPE_B_DB']) > 1:
            for row in raw_data['TYPE_B_DB'][1:]:
                if len(row) >= 13:
                    # USER CONFIG: Modify column indices according to your secondary data structure
                    record_id = clean_key(row[4])  # Column E (index 4) - modify as needed
                    if record_id:
                        secondary_db_mapping[record_id] = {
                            'orgId': clean_text(row[1]),  # Column B (index 1) - modify as needed
                            'SOURCE1': convert_data_status(row[6]),   # Column G - modify as needed
                            'SOURCE2': convert_data_status(row[7]),   # Column H - modify as needed
                            'SOURCE3': convert_data_status(row[8]),   # Column I - modify as needed
                            'DATA4': convert_data_status(row[9]),     # Column J - modify as needed
                            'DATA3': convert_data_status(row[10]),    # Column K - modify as needed
                            'DATA5': convert_data_status(row[11]),    # Column L - modify as needed
                            'DATA1': convert_data_status(row[12])     # Column M - modify as needed
                        }
        print(f"      {len(secondary_db_mapping):,} secondary database records processed")
        
        # Process secondary define submissions
        print("   Processing secondary define submissions...")
        secondary_define_mapping = {source: {} for source in SECONDARY_SOURCES}
        if len(raw_data['TYPE_B_DEFINE']) > 1:
            for row in raw_data['TYPE_B_DEFINE'][1:]:
                if len(row) >= 27:
                    for source_name in SECONDARY_SOURCES:
                        schema = SECONDARY_DEFINE_SCHEMA[source_name]
                        key_column = schema['key']
                        submission_column = schema['submission']
                        response_column = schema['response']
                        
                        if key_column < len(row):
                            record_id = clean_key(row[key_column])
                            if record_id:
                                submission = format_secondary_date(row[submission_column]) if submission_column < len(row) else ''
                                response = format_secondary_date(row[response_column]) if response_column < len(row) else ''
                                
                                if submission or response:
                                    secondary_define_mapping[source_name][record_id] = {
                                        'submission_date': submission,
                                        'response_date': response
                                    }
        
        secondary_define_total = sum(len(secondary_define_mapping[source]) for source in SECONDARY_SOURCES)
        print(f"      {secondary_define_total:,} secondary define submission records processed")
        
        # Process sales status data
        print("   Processing sales status data...")
        sales_status_mapping = {}
        if len(raw_data['TYPE_A_DATA']) > 1:
            for row in raw_data['TYPE_A_DATA'][1:]:
                if len(row) >= 4:
                    # USER CONFIG: Modify column indices according to your sales data structure
                    record_id = clean_key(row[3])  # Column D (index 3) - modify as needed
                    if record_id:
                        sales_status_raw = row[1] if len(row) > 1 else ''  # Column B (index 1) - modify as needed
                        sales_status_converted = convert_sales_status(sales_status_raw)
                        sales_status_mapping[record_id] = {
                            'sales_status': sales_status_converted,
                            'record_type': 'TYPE_A'
                        }
        print(f"      {len(sales_status_mapping):,} TYPE_A records processed")
        
        # Process main data submissions
        print("   Processing main data submissions...")
        data_mapping = {source: {} for source in DATA_SOURCES}
        if len(raw_data['BG']) > 1:
            for row in raw_data['BG'][1:]:
                if len(row) >= 29:
                    for source_name in DATA_SOURCES:
                        schema = DATA_SUBMISSION_SCHEMA[source_name]
                        key_column = schema['key']
                        data_columns = schema['cols']
                        
                        if key_column < len(row):
                            record_id = clean_key(row[key_column])
                            if record_id and any(col < len(row) and row[col] for col in data_columns):
                                data_mapping[source_name][record_id] = {
                                    'submission_date': combine_date_time(
                                        row[data_columns[0]] if data_columns[0] < len(row) else '',
                                        row[data_columns[1]] if data_columns[1] < len(row) else ''
                                    ),
                                    'response_date': combine_date_time(
                                        row[data_columns[2]] if data_columns[2] < len(row) else '',
                                        row[data_columns[3]] if data_columns[3] < len(row) else ''
                                    )
                                }
        
        data_total = sum(len(data_mapping[source]) for source in DATA_SOURCES)
        print(f"      {data_total:,} main data submission records processed")
        
        # Process cancellation submissions
        print("   Processing cancellation submissions...")
        cancellation_mapping = {source: {} for source in DATA_SOURCES}
        if len(raw_data['IG']) > 1:
            for row in raw_data['IG'][1:]:
                if len(row) >= 29:
                    for source_name in DATA_SOURCES:
                        schema = CANCELLATION_SCHEMA[source_name]
                        key_column = schema['key']
                        data_columns = schema['cols']
                        
                        if key_column < len(row):
                            record_id = clean_key(row[key_column])
                            if record_id and any(col < len(row) and row[col] for col in data_columns):
                                cancellation_mapping[source_name][record_id] = {
                                    'submission_date': combine_date_time(
                                        row[data_columns[0]] if data_columns[0] < len(row) else '',
                                        row[data_columns[1]] if data_columns[1] < len(row) else ''
                                    ),
                                    'response_date': combine_date_time(
                                        row[data_columns[2]] if data_columns[2] < len(row) else '',
                                        row[data_columns[3]] if data_columns[3] < len(row) else ''
                                    )
                                }
        
        cancellation_total = sum(len(cancellation_mapping[source]) for source in DATA_SOURCES)
        print(f"      {cancellation_total:,} cancellation records processed")
        
        # Process device data
        print("   Processing device data...")
        device_mapping = {}
        if len(raw_data['SECONDARY']) > 1:
            for row in raw_data['SECONDARY'][1:]:
                if len(row) >= 11:
                    # USER CONFIG: Modify column indices according to your device data structure
                    record_id = clean_key(row[0])    # Column A (index 0) - modify as needed
                    source_name = clean_key(row[8])  # Column I (index 8) - modify as needed
                    if record_id and source_name:
                        combined_key = f"{record_id}|{source_name}"
                        device_mapping[combined_key] = {
                            'brand': clean_text(row[1]),        # Column B - modify as needed
                            'model': clean_text(row[2]),        # Column C - modify as needed
                            'deviceId': clean_text(row[3]),     # Column D - modify as needed
                            'serialNumber': clean_text(row[4]), # Column E - modify as needed
                            'provider': clean_text(row[5]),     # Column F - modify as needed
                            'lastContact': clean_text(row[6]),  # Column G - modify as needed
                            'deviceStatus': clean_text(row[7]), # Column H - modify as needed
                            'sourceId': clean_text(row[9]) if len(row) > 9 else '',   # Column J - modify as needed
                            'deviceCode': clean_text(row[10]) if len(row) > 10 else '' # Column K - modify as needed
                        }
        print(f"      {len(device_mapping):,} device records processed")
        
        processing_duration = time.time() - processing_start
        print(f"   Data processing duration: {processing_duration:.1f} seconds")
        
        print("\n7. Creating historic rows...")
        historic_start = time.time()
        
        historic_rows = []
        
        # Collect all unique record IDs
        print("   Identifying unique record IDs...")
        all_record_ids = set()
        
        # Add record IDs from main DB
        for record_id in db_mapping.keys():
            all_record_ids.add(record_id)
        
        # Add record IDs from secondary DB
        for record_id in secondary_db_mapping.keys():
            all_record_ids.add(record_id)
        
        # Add record IDs from device data
        for key in device_mapping.keys():
            record_id = key.split('|')[0]
            all_record_ids.add(record_id)
        
        # Add record IDs from sales status data
        for record_id in sales_status_mapping.keys():
            all_record_ids.add(record_id)
        
        # Add record IDs from data submissions
        for source in DATA_SOURCES:
            for record_id in data_mapping[source].keys():
                all_record_ids.add(record_id)
        
        # Add record IDs from cancellation submissions
        for source in DATA_SOURCES:
            for record_id in cancellation_mapping[source].keys():
                all_record_ids.add(record_id)
        
        # Add record IDs from secondary define submissions
        for source in SECONDARY_SOURCES:
            for record_id in secondary_define_mapping[source].keys():
                all_record_ids.add(record_id)
        
        all_record_ids = sorted(list(all_record_ids))
        print(f"   Total unique record IDs: {len(all_record_ids):,}")
        
        # Create historic rows
        print("   Creating historic rows...")
        processed_count = 0
        
        for record_id in all_record_ids:
            # Process from main DB for TYPE_A
            if record_id in db_mapping:
                db_record = db_mapping[record_id]
                for source_name in DATA_SOURCES:
                    status = db_record.get(source_name, 'Not Sent')
                    org_id = db_record.get('orgId', '')
                    
                    data_record = data_mapping[source_name].get(record_id, {})
                    cancellation_record = cancellation_mapping[source_name].get(record_id, {})
                    
                    device_key = f"{record_id}|{source_name}"
                    device_record = device_mapping.get(device_key, {})
                    
                    sales_status_record = sales_status_mapping.get(record_id, {})
                    
                    cancellation_submission = cancellation_record.get('submission_date', '')
                    cancellation_response = cancellation_record.get('response_date', '')
                    request_type = determine_request_type(cancellation_submission, cancellation_response)
                    
                    final_device_status = device_record.get('deviceStatus', '') if device_record else ''
                    final_record_type = sales_status_record.get('record_type', 'TYPE_A') if sales_status_record else 'TYPE_A'
                    
                    data_exists = (
                        status != 'Not Sent' or
                        data_record or
                        cancellation_record or
                        device_record or
                        sales_status_record or
                        org_id
                    )
                    
                    if data_exists:
                        row = [
                            record_id, org_id, source_name, status,
                            data_record.get('submission_date', ''),
                            data_record.get('response_date', ''),
                            cancellation_record.get('submission_date', ''),
                            cancellation_record.get('response_date', ''),
                            request_type,
                            sales_status_record.get('sales_status', ''),
                            final_record_type, final_device_status,
                            device_record.get('brand', '') if device_record else '',
                            device_record.get('model', '') if device_record else '',
                            device_record.get('deviceId', '') if device_record else '',
                            device_record.get('serialNumber', '') if device_record else '',
                            device_record.get('provider', '') if device_record else '',
                            device_record.get('lastContact', '') if device_record else '',
                            device_record.get('sourceId', '') if device_record else '',
                            device_record.get('deviceCode', '') if device_record else ''
                        ]
                        historic_rows.append(row)
            
            # Process from secondary DB for TYPE_B
            if record_id in secondary_db_mapping:
                secondary_record = secondary_db_mapping[record_id]
                for source_name in SECONDARY_SOURCES:
                    status = secondary_record.get(source_name, 'Not Sent')
                    org_id = secondary_record.get('orgId', '')
                    
                    secondary_define = secondary_define_mapping[source_name].get(record_id, {})
                    device_key = f"{record_id}|{source_name}"
                    device_record = device_mapping.get(device_key, {})
                    sales_status_record = sales_status_mapping.get(record_id, {})
                    
                    final_device_status = device_record.get('deviceStatus', '') if device_record else ''
                    final_record_type = 'TYPE_B'
                    if sales_status_record and sales_status_record.get('record_type'):
                        final_record_type = sales_status_record['record_type']
                    
                    data_exists = (
                        status != 'Not Sent' or
                        secondary_define or
                        device_record or
                        sales_status_record or
                        org_id
                    )
                    
                    if data_exists:
                        row = [
                            record_id, org_id, source_name, status,
                            secondary_define.get('submission_date', ''),
                            secondary_define.get('response_date', ''), '', '',
                            'Installation',
                            sales_status_record.get('sales_status', ''),
                            final_record_type, final_device_status,
                            device_record.get('brand', '') if device_record else '',
                            device_record.get('model', '') if device_record else '',
                            device_record.get('deviceId', '') if device_record else '',
                            device_record.get('serialNumber', '') if device_record else '',
                            device_record.get('provider', '') if device_record else '',
                            device_record.get('lastContact', '') if device_record else '',
                            device_record.get('sourceId', '') if device_record else '',
                            device_record.get('deviceCode', '') if device_record else ''
                        ]
                        historic_rows.append(row)
            
            processed_count += 1
            if processed_count % 25000 == 0:
                print(f"   Processed: {processed_count:,}/{len(all_record_ids):,} - Historic: {len(historic_rows):,}")
        
        # Alphabetical sorting
        print("   Performing alphabetical sorting...")
        historic_rows.sort(key=lambda x: (x[0], x[2]))
        
        historic_duration = time.time() - historic_start
        print(f"   {len(historic_rows):,} historic rows prepared")
        print(f"   Historic creation time: {historic_duration:.1f} seconds")
        
        # Statistics
        installation_count = sum(1 for row in historic_rows if row[8] == 'Installation')
        deletion_count = sum(1 for row in historic_rows if row[8] == 'Deletion')
        type_a_count = sum(1 for row in historic_rows if row[10] == 'TYPE_A')
        type_b_count = sum(1 for row in historic_rows if row[10] == 'TYPE_B')
        status1_count = sum(1 for row in historic_rows if row[9] == 'Status1')
        status2_count = sum(1 for row in historic_rows if row[9] == 'Status2')
        completed_count = sum(1 for row in historic_rows if row[3] == 'Completed')
        rejected_count = sum(1 for row in historic_rows if row[3] == 'Rejected')
        awaiting_count = sum(1 for row in historic_rows if row[3] == 'Awaiting Response')
        not_sent_count = sum(1 for row in historic_rows if row[3] == 'Not Sent')
        
        print(f"   Request Type: {installation_count:,} Installation, {deletion_count:,} Deletion")
        print(f"   Record Type: {type_a_count:,} TYPE_A, {type_b_count:,} TYPE_B")
        print(f"   Sales Status: {status1_count:,} Status1, {status2_count:,} Status2")
        print(f"   Data Status: {completed_count:,} Completed, {rejected_count:,} Rejected, {awaiting_count:,} Awaiting Response, {not_sent_count:,} Not Sent")
        
        # Memory cleanup
        del raw_data, db_mapping, secondary_db_mapping, data_mapping, cancellation_mapping, device_mapping, sales_status_mapping, secondary_define_mapping
        gc.collect()
        
        print("\n8. Creating Jupyter local Excel files...")
        excel_start = time.time()
        
        # USER CONFIG: Modify column headers according to your requirements
        column_headers = [
            'Record ID', 'Organization ID', 'Data Source', 'Processing Status',
            'Submission Date', 'Response Date', 'Cancellation Submission', 'Cancellation Response',
            'Request Type', 'Sales Status', 'Record Type', 'Device Status',
            'Brand', 'Model', 'Device ID', 'Serial Number', 'Provider',
            'Last Contact', 'Source ID', 'Device Code'
        ]
        
        # USER CONFIG: Modify chunk size (rows per Excel file)
        CHUNK_SIZE = 100000  # Options: 50000, 100000, 150000, 200000
        total_rows = len(historic_rows)
        total_chunks = max(1, (total_rows + CHUNK_SIZE - 1) // CHUNK_SIZE)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        
        print(f"   Total {total_rows:,} rows")
        print(f"   {CHUNK_SIZE:,} rows per file = {total_chunks} Excel files")
        print(f"   Platform: Jupyter Notebook (Local)")
        
        successful_files = 0
        total_file_size = 0
        file_list = []
        
        # Batch file creation for Jupyter
        for chunk_index in range(total_chunks):
            start_idx = chunk_index * CHUNK_SIZE
            end_idx = min(start_idx + CHUNK_SIZE, total_rows)
            chunk_data = historic_rows[start_idx:end_idx]
            chunk_size_actual = len(chunk_data)
            
            file_number = chunk_index + 1
            
            print(f"\n   Creating Excel File {file_number}/{total_chunks}")
            print(f"      Row range: {start_idx + 1:,} - {end_idx:,}")
            print(f"      Rows in this file: {chunk_size_actual:,}")
            
            if chunk_size_actual == 0:
                continue
            
            # USER CONFIG: Modify output file naming pattern
            excel_file_name = f"data_management_system{file_number:02d}_{timestamp}.xlsx"
            
            # Create file
            success, file_size = create_excel_file_jupyter(chunk_data, excel_file_name, column_headers)
            
            if success:
                successful_files += 1
                total_file_size += file_size
                file_list.append(excel_file_name)
            else:
                print(f"      Failed to create {excel_file_name}")
        
        excel_duration = time.time() - excel_start
        total_duration = time.time() - GLOBAL_START
        
        print("\n" + "=" * 60)
        print("JUPYTER DATA MANAGEMENT SYSTEM COMPLETED SUCCESSFULLY")
        print("=" * 60)
        
        print(f"FINAL RESULTS:")
        print(f"   • Historic Rows: {len(historic_rows):,}")
        print(f"   • Excel Columns: {len(column_headers)} (20 columns)")
        print(f"   • Total Cells: {len(historic_rows) * len(column_headers):,}")
        print(f"   • Excel File Count: {successful_files}")
        print(f"   • Total File Size: {total_file_size:.1f} MB")
        print(f"   • Average File Size: {total_file_size/successful_files:.1f} MB" if successful_files > 0 else "")
        print(f"   • Platform: Jupyter Notebook (Local)")
        print(f"")
        
        print(f"CREATED FILES:")
        current_folder = os.getcwd()
        for i, file in enumerate(file_list, 1):
            if os.path.exists(file):
                size = os.path.getsize(file) / 1024 / 1024
                full_path = os.path.abspath(file)
                print(f"   {i:2d}. {file} ({size:.1f} MB) - SUCCESS")
                print(f"       Path: {full_path}")
            else:
                print(f"   {i:2d}. {file} - FAILED")
        
        print(f"\nDATA ANALYSIS:")
        print(f"   • Request Type: {installation_count:,} Installation, {deletion_count:,} Deletion")
        print(f"   • Record Type: {type_a_count:,} TYPE_A, {type_b_count:,} TYPE_B")
        print(f"   • Sales Status: {status1_count:,} Status1, {status2_count:,} Status2")
        print(f"   • Processing Status: {completed_count:,} Completed, {rejected_count:,} Rejected, {awaiting_count:,} Awaiting Response, {not_sent_count:,} Not Sent")
        
        print(f"\nPERFORMANCE METRICS:")
        print(f"   • Parallel Data Retrieval: {data_duration:.1f} seconds")
        print(f"   • Fast Data Processing: {processing_duration:.1f} seconds")
        print(f"   • Historic Creation: {historic_duration:.1f} seconds")
        print(f"   • Excel Creation: {excel_duration:.1f} seconds")
        print(f"   • TOTAL PROCESSING TIME: {total_duration:.1f} seconds ({total_duration/60:.1f} minutes)")
        
        print("=" * 60)
        print("JUPYTER DATA MANAGEMENT SYSTEM COMPLETED SUCCESSFULLY")
        
    except Exception as e:
        print(f"Processing error: {e}")

def create_excel_report():
    """Execute full Excel report generation"""
    print(f"\nSTARTING JUPYTER EXCEL REPORT...")
    create_data_management_excel_jupyter()

def quick_test():
    """Generate quick test file with sample data"""
    print(f"\nJUPYTER QUICK TEST - 1000 ROW SAMPLE")
    
    test_data = []
    for i in range(1000):
        status_examples = ['Completed', 'Rejected', 'Awaiting Response', 'Not Sent']
        status = status_examples[i % 4]
        
        test_data.append([
            f'TEST{i:06d}', f'ORG{i:06d}', 'DATA1', status,
            '26.08.2024 09:00', '26.08.2024 09:30', '', '',
            'Installation', 'Status2', 'TYPE_A', 'Active',
            'BRAND_A', 'MODEL_A', f'DEV{i:06d}', f'SER{i:09d}', 'PROVIDER_A',
            '26.08.2024 10:00', f'SRC{i:06d}', f'CODE{i:06d}'
        ])
    
    column_headers = [
        'Record ID', 'Organization ID', 'Data Source', 'Processing Status',
        'Submission Date', 'Response Date', 'Cancellation Submission', 'Cancellation Response',
        'Request Type', 'Sales Status', 'Record Type', 'Device Status',
        'Brand', 'Model', 'Device ID', 'Serial Number', 'Provider',
        'Last Contact', 'Source ID', 'Device Code'
    ]
    
    test_file = f"data_management_test_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx"
    
    success, file_size = create_excel_file_jupyter(test_data, test_file, column_headers)
    
    if success:
        print(f"   Test file created successfully: {test_file}")
        print(f"   File location: {os.path.abspath(test_file)}")
    else:
        print(f"   Failed to create test file")

print(f"\n" + "="*60)
print(f"JUPYTER DATA MANAGEMENT SYSTEM READY")
print(f"="*60)
print(f"USAGE OPTIONS:")
print(f"")
print(f"1. QUICK TEST (1000 row sample):")
print(f"   quick_test()")
print(f"")
print(f"2. FULL EXCEL REPORT (local save):")
print(f"   create_excel_report()")
print(f"")
print(f"CONFIGURATION REQUIREMENTS:")
print(f"   1. Replace YOUR_PROJECT_ID with your actual project ID")
print(f"   2. Replace YOUR_PRIVATE_KEY with your actual private key")
print(f"   3. Replace YOUR_CLIENT_EMAIL with your service account email")
print(f"   4. Replace GOOGLESHEET1_ID, GOOGLESHEET2_ID, GOOGLESHEET3_ID")
print(f"   5. Replace sheet names: MainData, Submissions, Cancellations, etc.")
print(f"   6. Invite service account email to your Google Sheets as Editor")
print(f"")
print(f"CUSTOMIZABLE SETTINGS:")
print(f"   • DATA_SOURCES: Modify data source names")
print(f"   • SECONDARY_SOURCES: Modify secondary source names")
print(f"   • CHUNK_SIZE: Change rows per Excel file")
print(f"   • column_headers: Customize Excel column names")
print(f"   • Schema mappings: Adjust column indices for your data structure")
print(f"")
print(f"START COMMANDS:")
print(f"   quick_test()           # For testing")
print(f"   create_excel_report()  # For full report")
print("="*60)